{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information, Bits and Binary Digits\n",
    "- Binary Digit: value of a binary variable, where this value can be either a 0 or 1. Binary Digit is not information *per se*\n",
    "- Bit: definite amount of information\n",
    "- Bits and Binary Digits are different types of entities\n",
    "- Consider the following examples\n",
    "  - Example 1\n",
    "    - If you already know that you should take the left-hand road from point A and I show you the binary digit 0 (=left), then\n",
    "    - you have been given a binary digit but you have gained no information\n",
    "  - Example 2\n",
    "    - at the other extreme, if you have no idea about which road to choose and I show you a 0, then\n",
    "    - you have been given a binary digit and you have also gained one bit of information\n",
    "  - Between these two extremes, if someone tells you there is a 71% probability that the left-hand road represents the correct decision, and I subsequently confirm this by showing you a 0 -> this 0 provides you with less than one bit of information\n",
    "    - because you already had some information about which road to choose\n",
    "    - in fact, when you receive 0 in this case, you actually get 0.5 bit \"attend later lectures\"\n",
    "    - Yes, we can use 1 binary digit to tell you 0.5 bit ðŸ™ƒ\n",
    "- Do you think we shall name the unit of information *Shannon* ?\n",
    "  - May be Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Point\n",
    "- A bit is the amount of informtion required to choose between two equally probable alternatives e.g. left/right)\n",
    "- whereas a binary digit is the value of a binary variable, which can adopt one of two possible values (i.e. 0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Telegraphy\n",
    "- Suppose that\n",
    "  - you have just discovered that if you hold a compass next to a wire, then the compass needle changes position when you pass a current through the wire\n",
    "  - If the wire is long enough to connect two towns like London and Manchester, then a current initiated in London can deflect a compass needle held near to the wire in Manchester\n",
    "  - You would like to use this new technology to send messages in the form of individual letters\n",
    "  - Sadly, the year is 1820, so you will have to wait over 100 years for Shannon'g paper to be published ðŸ™ƒ\n",
    "- Let's say\n",
    "  - You want to send only upper-case letters, to keep matters simple\n",
    "- So\n",
    "  - You set up 26 electric lines, one per letter from A to Z, with the first line being A, the second line being B, and so on\n",
    "  - Each line is set up next to a compass which is kept some distance from all the other lines, to prevent each line from deflecting more than one compass\n",
    "- In London\n",
    "  - each line is labelled with a letter, and the corresponding line is labelled with the same letter in Manchester\n",
    "- For example\n",
    "  - if you want to send the letter D, you press a switch on the fourth line in London, which sends an electric current to Machester along the wire which is next to the compass labelled with the letter D\n",
    "- Of course\n",
    "  - lines fail from time to time, and it is about 200 miles from London to Manchester, so finding the location of the break in a line is difficult and expensive\n",
    "  - Naturally, if there were fewer lines then there would be fewer failures\n",
    "- Cooke and Wheatstone\n",
    "  - both devised a complicated two-needle system, which could send only 23 different letters\n",
    "  - Despite the complexity of their system, it famously led to the arrest of a murderer ðŸ™ƒ\n",
    "- Story ðŸ™ƒ\n",
    "  - On the first of January 1845, John Tawell poisoned his mistress, Sarah Hart, in a place called Salt Hill in the county of Berkshire, before escaping in a trainto Paddington station in London\n",
    "  - In order to ensure Tawell's arrest when he reached his destination, the following telegraph was sent to London\n",
    "  \n",
    "**A MURDERER HAS GUST BEEN COMMITTED AT SALT HILL AND THE SUSPECTED MURDERER  WAS SEEN TO TAKE A FIRST CLASS TICKET TO LONDON BY THE TRAIN WHICH LEFT SLOUGH AT 742 PM HE IS IN THE GRAB OF KWAKER ...**\n",
    "\n",
    "  - The unusual spellings of the words JUST and QUAKER were a result of the telegrapher doing his best in the absence of the letters J, Q, and Z in the array of 23 letters before him\n",
    "  - As a result of this telegram, Tawell was arrested and subsequently hanged for murder\n",
    "  - The role of Cooke and Wheatstone's telegraph in Tawell's arrest was widely reported in the press, and established the practiality of telegraphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Telegraphy (cont.)\n",
    "- In the 1830s, Samuel Morse and Alfred Vail developed the first version of (what came to be known as) the Morse code.\n",
    "- Because this specified each letter as dots and dashes, it could be used to send messages over a single line.\n",
    "- An important property of Morse code is that it uses short codewords for the most common letters, and longer codewords for less common letters\n",
    "- Morse adopted a simple strategy to find out which letters were most common. \n",
    "- Reasoning that newspaper printers would have only as many copies of each letter as were required, he went to a printerâ€™s workshop and counted the copies of each letter.\n",
    "- As a result, the most common letter E is specified as a single dot, whereas the rare J is specified as a dot followed by three dashes.\n",
    "- The ingenious strategy adopted by Morse is important because it enables efficient use of the communication channel (a single wire). \n",
    "- We will return to this theme many times, and it raises a fundamental question: \n",
    "  - how can we tell if a communication channel is being used as efficiently as possible?\n",
    "- ![](imgs/Lecture-05/morse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Theory\n",
    "- Fundamental to understanding and characterizing the performance of communication systems\n",
    "- Evolved to variety of different applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/Lecture-05/comm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- Source: produces message signals\n",
    "- Encoder: encodes message signals to produce a stream of bits\n",
    "- These bits are going to be streamed over a channel\n",
    "- This transmission requires bandwidth (usage of the channel)\n",
    "- Technically: we like to minimize the usage of the channel\n",
    "  - so, for a given amount of resources (channel bandwidth)\n",
    "  - we want to transmit the maximum amount of information \n",
    "    - either from point a to point b\n",
    "    - or through broadcast\n",
    "- Objective: \n",
    "  - **Convey Maximum possible information using Minimum amount of resources**\n",
    "  - Resources here means\n",
    "    - Time\n",
    "    - Bandwidth\n",
    "  - In digital communicatio terms:\n",
    "    - Minimum number of bits encapsulating the maximum amount of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What?\n",
    "- Let's rephrase the Question\n",
    "  - We would like to encode or represent each message using the fewest possible number of bits\n",
    "  - Compress? Maybe :)\n",
    "  \n",
    "- Information Theory is a systematic study that allows us to compress information -> Not official Definition\n",
    "- Information Theory is a systematic study that allows us to compress information **Efficiently** -> Better, however not final :)\n",
    "- Maximize information per bit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How?\n",
    "- We can Characterize Physical attributes, such as\n",
    "  - Weight\n",
    "  - Height\n",
    "  - Volume\n",
    "  - etc.\n",
    "\n",
    "- How can we characterize something abstract; such as Information Content?\n",
    "  - For sure, we can feel and detect that\n",
    "  - When we listen to news, we can identify some news are valuable than others\n",
    "  - same with websites, articles, books, several sources of information, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Theory\n",
    "- provides a tangible measure of information content of a source\n",
    "- Mathematical Framework \n",
    "- Objective Measure\n",
    "- Quantitative Framework\n",
    "- Source of Information\n",
    "  - Central to Information Theory\n",
    "  - S\n",
    "  - we will focus now on Discrete source \n",
    "  - we will delay Continous sources for later discussion\n",
    "  - Generates discrete set of   **Sybmols** \n",
    "    - $S_{0}, S{1}, .., S_{m-1}$\n",
    "    - Known as: \n",
    "      - Source of Alphabet\n",
    "      - Symbol Set\n",
    "      - $S = \\{S_{0}, S{1}, .., S_{m-1}\\}$\n",
    "  - Symbols are generated according to a probability distribution\n",
    "    - $P(S_{0})=P_{0}$\n",
    "    - $P(S_{1})=P_{1}$\n",
    "    - ..\n",
    "    - ..\n",
    "    - $P(S_{m-1})=P_{m-1}$\n",
    "  - Axioms of Probability\n",
    "    - $P_{i} \\geq 0$\n",
    "    - $\\sum_{i=0}^{m-1}P(_{i})=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Content\n",
    "How to charactertize information content of a symbol?\n",
    "- Define Information of Symbol $s_{i}$\n",
    "- $I(s_{i}) = log_{2}\\frac{1}{P(s_{i})} = log_{2}\\frac{1}{P_{i}}$ bits\n",
    "- This is known as Information of Content\n",
    "- Information of the symbol has nothing to do with how a symbol itself has to represented\n",
    "- It depends on the probability $P_{i}$ of the current symbol\n",
    "- Further: $I(s_{i}) \\propto \\frac{1}{P_{i}}$\n",
    "- If $\\lim_{P_{i} \\to 1}$ then $\\lim_I(s_{i}) \\to log_{2}1$\n",
    "- If $\\lim_{P_{i} \\to 0}$ then $\\lim_I(s_{i}) \\to \\infty$\n",
    "- $\\frac{1}{p_{i}}$ is large -> means that such events are infrequent\n",
    "- Translation\n",
    "  - Symbols or events with very low probability of occurence, have higher information content\n",
    "  - Rare Events = Higher Information\n",
    "- Example\n",
    "  - News coverage focus on such rare events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entropy\n",
    "- $I(s_{i}) = log_{2}(\\frac{1}{P_{i}})$ \n",
    "- bits\n",
    "- $P_{i} = Pr(s_{i})$\n",
    "- Average Information of source $S$\n",
    "  - $I(S) = E\\{I(s)\\} = \\sum_{i=0}^{M-1} P_{i} I(s_{i}) = \\sum_{i=0}^{M-1}P_{i}log_{2}(\\frac{1}{P_{i}}) = H(S)$\n",
    "- ![](imgs/Lecture-04/shannon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of Source\n",
    "- characterizes - quantifies - the average information content of the source (average information per symbol emitted/generated by source).\n",
    "- Average uncertainity associated with source\n",
    "- More uncertainity = More information\n",
    "- Important Proof\n",
    "  1. $H(S) = \\sum_{i=0}^{M-1}P_{i}log_{2}(\\frac{1}{P_{i}})$ \n",
    "  - $0 \\leq P_{i} \\leq 1$\n",
    "  - $1 \\leq \\frac{1}{P_{i}} \\leq \\infty $\n",
    "  - $log_{2}(\\frac{1}{P_{i}}) \\geq 1$\n",
    "  - $P_{i}log_{2}(\\frac{1}{P_{i}}) \\leq 0 \\forall i$\n",
    "- Sum of Non-Negative Terms $H(S) \\geq 0$\n",
    "- Entropy is Non-Negative, it can be zero however $$H(S) \\geq 0$$\n",
    "- Further, if $P_{i} = 1 $ then $P_{i}log_{2}(\\frac{1}{P_{i}}) = 1 * log_{2}(\\frac{1}{1}) = 0$ for $p_{i} = 1$\n",
    "- Further, if $P_{i} = 0 $ then ?\n",
    "  - we need some calculus, as the numerator and denomiator tends to infinity as $P_{i}$ tends to zero\n",
    "  - out of scope (at least for me ðŸ˜€ )\n",
    "  - Equals also to Zero\n",
    "- Entropy approximately equal to zero for very common and very rare events !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy - Example\n",
    "- Given a Binary Source, where alphabet consists of two symbols: 0 and 1\n",
    "- $S=\\{ S_{0}, S_{1}\\}$\n",
    "- $P(S_{0}) = P$\n",
    "- $P(S_{1}) = 1-P$\n",
    "- $H(S) = P(S_{0}) log_{2}\\frac{1}{P(S_{0})} + P(S_{1}) log_{2}\\frac{1}{P(S_{1})}$ = $Plog_{2}\\frac{1}{P} + (1-P)log_{2}\\frac{1}{1-P}$\n",
    "- $H(1-P) = H(P)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
